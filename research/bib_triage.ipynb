{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5dda5f6",
   "metadata": {},
   "source": [
    "# BibTeX Triage for ACM Exports\n",
    "\n",
    "This notebook:\n",
    "- Counts BibTeX entries per `.bib` file\n",
    "- Flags entries missing DOIs\n",
    "- Deduplicates (by DOI if present, else by title+year)\n",
    "- Filters likely-relevant candidates by keywords\n",
    "- Exports a reviewable CSV: `bib_candidates.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30ebe30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Update these filenames if yours differ\n",
    "BIB_FILES = [\n",
    "    \"CIKM.bib\",\n",
    "    \"KDD.bib\",\n",
    "    \"RecSys.bib\",\n",
    "    \"TheWebConf.bib\",\n",
    "    \"WSDM.bib\",\n",
    "]\n",
    "\n",
    "# Tune these to your project framing\n",
    "INCLUDE_KEYWORDS = [\n",
    "    \"popularity\",\n",
    "    \"engagement\",\n",
    "    \"rating\",\n",
    "    \"ratings\",\n",
    "    \"vote\",\n",
    "    \"votes\",\n",
    "    \"attention\",\n",
    "    \"prediction\",\n",
    "    \"predict\",\n",
    "    \"response\",\n",
    "    \"collective\",\n",
    "]\n",
    "\n",
    "# Exclude obvious off-topic domains (sensors, biomedical, etc.)\n",
    "EXCLUDE_KEYWORDS = [\n",
    "    \"physiological\",\n",
    "    \"thermal\",\n",
    "    \"ecg\",\n",
    "    \"eeg\",\n",
    "    \"wearable\",\n",
    "    \"biosignal\",\n",
    "    \"medical\",\n",
    "    \"diagnosis\",\n",
    "    \"patient\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658a7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRY_START_RE = re.compile(r\"^\\s*@\\w+\\s*{\\s*([^,]+)\\s*,\\s*$\", re.IGNORECASE)\n",
    "FIELD_RE = re.compile(r\"^\\s*(\\w+)\\s*=\\s*[{\\\"](.+?)[}\\\"]\\s*,?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def split_entries(text: str):\n",
    "    \"\"\"\n",
    "    Simple BibTeX splitter: assumes entries start with '@' at the beginning of a line.\n",
    "    Works well for standard ACM DL exports.\n",
    "    \"\"\"\n",
    "    chunks = re.split(r\"\\n(?=@)\", text.strip(), flags=re.MULTILINE)\n",
    "    return [c.strip() for c in chunks if c.strip().startswith(\"@\")]\n",
    "\n",
    "def parse_entry(entry_text: str):\n",
    "    \"\"\"\n",
    "    Parses a BibTeX entry into a dict with common fields.\n",
    "    This is intentionally lightweight (no external packages).\n",
    "    \"\"\"\n",
    "    lines = entry_text.splitlines()\n",
    "    m = ENTRY_START_RE.match(lines[0])\n",
    "    key = m.group(1).strip() if m else \"\"\n",
    "\n",
    "    fields = {}\n",
    "    for line in lines[1:]:\n",
    "        fm = FIELD_RE.match(line)\n",
    "        if fm:\n",
    "            fname = fm.group(1).lower()\n",
    "            fval = fm.group(2).strip()\n",
    "            fields[fname] = fval\n",
    "\n",
    "    title = fields.get(\"title\", \"\")\n",
    "    year = fields.get(\"year\", \"\")\n",
    "    doi = fields.get(\"doi\", \"\")\n",
    "    booktitle = fields.get(\"booktitle\", \"\")\n",
    "    journal = fields.get(\"journal\", \"\")\n",
    "    venue = journal or booktitle\n",
    "\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"title\": title,\n",
    "        \"year\": year,\n",
    "        \"doi\": doi,\n",
    "        \"venue\": venue,\n",
    "        \"raw\": entry_text,\n",
    "    }\n",
    "\n",
    "def has_any_keyword(text: str, keywords):\n",
    "    t = text.lower()\n",
    "    return any(k.lower() in t for k in keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de96f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All bib files found.\n"
     ]
    }
   ],
   "source": [
    "missing = [f for f in BIB_FILES if not Path(f).exists()]\n",
    "if missing:\n",
    "    print(\"Missing files:\")\n",
    "    for f in missing:\n",
    "        print(\"  -\", f)\n",
    "else:\n",
    "    print(\"All bib files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca379d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per file:\n",
      "  CIKM.bib: 310\n",
      "  KDD.bib: 245\n",
      "  RecSys.bib: 379\n",
      "  TheWebConf.bib: 304\n",
      "  WSDM.bib: 135\n",
      "\n",
      "Total entries scanned (including duplicates across files): 1373\n",
      "Entries missing DOI: 29\n",
      "Candidate count (post-dedup): 312\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "missing_doi_rows = []\n",
    "candidates = []\n",
    "\n",
    "doi_seen = set()\n",
    "title_year_seen = set()\n",
    "\n",
    "per_file_counts = {}\n",
    "\n",
    "for fname in BIB_FILES:\n",
    "    path = Path(fname)\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    entries = split_entries(text)\n",
    "    per_file_counts[fname] = len(entries)\n",
    "\n",
    "    for e in entries:\n",
    "        item = parse_entry(e)\n",
    "\n",
    "        # Dedup logic\n",
    "        doi_norm = item[\"doi\"].strip().lower()\n",
    "        title_norm = re.sub(r\"\\s+\", \" \", item[\"title\"].strip().lower())\n",
    "        ty_norm = (title_norm, item[\"year\"].strip())\n",
    "\n",
    "        is_dup = False\n",
    "        if doi_norm:\n",
    "            if doi_norm in doi_seen:\n",
    "                is_dup = True\n",
    "            doi_seen.add(doi_norm)\n",
    "        else:\n",
    "            if ty_norm in title_year_seen:\n",
    "                is_dup = True\n",
    "            title_year_seen.add(ty_norm)\n",
    "\n",
    "        # Track missing DOI\n",
    "        if not doi_norm:\n",
    "            missing_doi_rows.append(item)\n",
    "\n",
    "        # Candidate filter\n",
    "        blob = f\"{item['title']} {item['venue']}\"\n",
    "        include = has_any_keyword(blob, INCLUDE_KEYWORDS)\n",
    "        exclude = has_any_keyword(blob, EXCLUDE_KEYWORDS)\n",
    "\n",
    "        if include and not exclude and not is_dup:\n",
    "            candidates.append(item)\n",
    "\n",
    "        all_rows.append(item)\n",
    "\n",
    "print(\"Counts per file:\")\n",
    "for k, v in per_file_counts.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nTotal entries scanned (including duplicates across files): {len(all_rows)}\")\n",
    "print(f\"Entries missing DOI: {len(missing_doi_rows)}\")\n",
    "print(f\"Candidate count (post-dedup): {len(candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2125af11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\JanMc\\Dropbox\\Education\\_GitHub_coursework\\janmcconnellCityU-coursework\\DS687_CAPSTONE\\research\\bib_candidates.csv\n"
     ]
    }
   ],
   "source": [
    "out_csv = Path(\"bib_candidates.csv\")\n",
    "\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"year\", \"title\", \"venue\", \"doi\", \"key\"])\n",
    "    w.writeheader()\n",
    "    for c in sorted(candidates, key=lambda x: (x[\"year\"], x[\"title\"])):\n",
    "        w.writerow({k: c.get(k, \"\") for k in [\"year\", \"title\", \"venue\", \"doi\", \"key\"]})\n",
    "\n",
    "print(\"Wrote:\", out_csv.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88a65c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 | Forecasting the Buzz: Enriching Hashtag Popularity Prediction with LLM Reasoning | Proceedings of the 34th ACM International Conference on Information and Knowledge Management | 10.1145/3746252.3760970\n",
      "2022 | A Gumbel-based Rating Prediction Framework for Imbalanced Recommendation | Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management | 10.1145/3511808.3557341\n",
      "2019 | Neural Review Rating Prediction with User and Product Memory | Proceedings of the 28th ACM International Conference on Information and Knowledge Management | 10.1145/3357384.3358138\n",
      "2018 | Heterogeneous Neural Attentive Factorization Machine for Rating Prediction | Proceedings of the 27th ACM International Conference on Information and Knowledge Management | 10.1145/3269206.3271759\n",
      "2017 | Recipe Popularity Prediction with Deep Visual-Semantic Fusion | Proceedings of the 2017 ACM on Conference on Information and Knowledge Management | 10.1145/3132847.3133137\n",
      "2022 | FedCDR: Federated Cross-Domain Recommendation for Privacy-Preserving Rating Prediction | Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management | 10.1145/3511808.3557320\n",
      "2018 | Adversarial Training Model Unifying Feature Driven and Point Process Perspectives for Event Popularity Prediction | Proceedings of the 27th ACM International Conference on Information and Knowledge Management | 10.1145/3269206.3271714\n",
      "2019 | ARP: Aspect-aware Neural Review Rating Prediction | Proceedings of the 28th ACM International Conference on Information and Knowledge Management | 10.1145/3357384.3358086\n",
      "2015 | Video Popularity Prediction by Sentiment Propagation via Implicit Network | Proceedings of the 24th ACM International on Conference on Information and Knowledge Management | 10.1145/2806416.2806505\n",
      "2016 | Feature Driven and Point Process Approaches for Popularity Prediction | Proceedings of the 25th ACM International on Conference on Information and Knowledge Management | 10.1145/2983323.2983812\n",
      "2013 | On popularity prediction of videos shared in online social networks | Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management | 10.1145/2505515.2505523\n",
      "2021 | DCAP: Deep Cross Attentional Product Network for User Response Prediction | Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management | 10.1145/3459637.3482246\n",
      "2013 | Review rating prediction based on the content and weighting strong social relation of reviewers | Proceedings of the 2013 International Workshop on Mining Unstructured Big Data Using Natural Language Processing | 10.1145/2513549.2513554\n",
      "2011 | Bayesian latent variable models for collaborative item rating prediction | Proceedings of the 20th ACM International Conference on Information and Knowledge Management | 10.1145/2063576.2063680\n",
      "2016 | Why Did You Cover That Song? Modeling N-th Order Derivative Creation with Content Popularity | Proceedings of the 25th ACM International on Conference on Information and Knowledge Management | 10.1145/2983323.2983674\n",
      "2017 | LARM: A Lifetime Aware Regression Model for Predicting YouTube Video Popularity | Proceedings of the 2017 ACM on Conference on Information and Knowledge Management | 10.1145/3132847.3132997\n",
      "2023 | Capturing Popularity Trends: A Simplistic Non-Personalized Approach for Enhanced Item Recommendation | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management | 10.1145/3583780.3614801\n",
      "2023 | Popularity-aware Distributionally Robust Optimization for Recommendation System | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management | 10.1145/3583780.3615492\n",
      "2017 | DeepHawkes: Bridging the Gap between Prediction and Understanding of Information Cascades | Proceedings of the 2017 ACM on Conference on Information and Knowledge Management | 10.1145/3132847.3132973\n",
      "2014 | Predicting the Popularity of Online Serials with Autoregressive Models | Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management | 10.1145/2661829.2662055\n",
      "2023 | PopDCL: Popularity-aware Debiased Contrastive Loss for Collaborative Filtering | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management | 10.1145/3583780.3615009\n",
      "2019 | Leveraging Ratings and Reviews with Gating Mechanism for Recommendation | Proceedings of the 28th ACM International Conference on Information and Knowledge Management | 10.1145/3357384.3357919\n",
      "2020 | A GAN-based Framework for Modeling Hashtag Popularity Dynamics Using Assistive Information | Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management | 10.1145/3340531.3412025\n",
      "2023 | Test-Time Embedding Normalization for Popularity Bias Mitigation | Proceedings of the 32nd ACM International Conference on Information and Knowledge Management | 10.1145/3583780.3615281\n",
      "2017 | Modeling Affinity based Popularity Dynamics | Proceedings of the 2017 ACM on Conference on Information and Knowledge Management | 10.1145/3132847.3132923\n"
     ]
    }
   ],
   "source": [
    "# Show a quick sample in the notebook\n",
    "for c in candidates[:25]:\n",
    "    print(f\"{c['year']} | {c['title']} | {c['venue']} | {c['doi']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1614938f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\Users\\JanMc\\Dropbox\\Education\\_GitHub_coursework\\janmcconnellCityU-coursework\\DS687_CAPSTONE\\research\\bib_missing_doi.csv\n"
     ]
    }
   ],
   "source": [
    "out_missing = Path(\"bib_missing_doi.csv\")\n",
    "with out_missing.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"year\", \"title\", \"venue\", \"key\"])\n",
    "    w.writeheader()\n",
    "    for m in sorted(missing_doi_rows, key=lambda x: (x[\"year\"], x[\"title\"])):\n",
    "        w.writerow({k: m.get(k, \"\") for k in [\"year\", \"title\", \"venue\", \"key\"]})\n",
    "\n",
    "print(\"Wrote:\", out_missing.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
