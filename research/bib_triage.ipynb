{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5dda5f6",
   "metadata": {},
   "source": [
    "# BibTeX Triage for ACM Exports\n",
    "\n",
    "This notebook:\n",
    "- Counts BibTeX entries per `.bib` file\n",
    "- Flags entries missing DOIs\n",
    "- Deduplicates (by DOI if present, else by title+year)\n",
    "- Filters likely-relevant candidates by keywords\n",
    "- Exports a reviewable CSV: `bib_candidates.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebe30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Update these filenames if yours differ\n",
    "BIB_FILES = [\n",
    "    \"CIKM.bib\",\n",
    "    \"KDD.bib\",\n",
    "    \"RecSys.bib\",\n",
    "    \"TheWebConf.bib\",\n",
    "    \"WSDM.bib\",\n",
    "]\n",
    "\n",
    "# Tune these to your project framing\n",
    "INCLUDE_KEYWORDS = [\n",
    "    \"popularity\",\n",
    "    \"engagement\",\n",
    "    \"rating\",\n",
    "    \"ratings\",\n",
    "    \"vote\",\n",
    "    \"votes\",\n",
    "    \"attention\",\n",
    "    \"prediction\",\n",
    "    \"predict\",\n",
    "    \"response\",\n",
    "    \"collective\",\n",
    "]\n",
    "\n",
    "# Exclude obvious off-topic domains (sensors, biomedical, etc.)\n",
    "EXCLUDE_KEYWORDS = [\n",
    "    \"physiological\",\n",
    "    \"thermal\",\n",
    "    \"ecg\",\n",
    "    \"eeg\",\n",
    "    \"wearable\",\n",
    "    \"biosignal\",\n",
    "    \"medical\",\n",
    "    \"diagnosis\",\n",
    "    \"patient\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTRY_START_RE = re.compile(r\"^\\s*@\\w+\\s*{\\s*([^,]+)\\s*,\\s*$\", re.IGNORECASE)\n",
    "FIELD_RE = re.compile(r\"^\\s*(\\w+)\\s*=\\s*[{\\\"](.+?)[}\\\"]\\s*,?\\s*$\", re.IGNORECASE)\n",
    "\n",
    "def split_entries(text: str):\n",
    "    \"\"\"\n",
    "    Simple BibTeX splitter: assumes entries start with '@' at the beginning of a line.\n",
    "    Works well for standard ACM DL exports.\n",
    "    \"\"\"\n",
    "    chunks = re.split(r\"\\n(?=@)\", text.strip(), flags=re.MULTILINE)\n",
    "    return [c.strip() for c in chunks if c.strip().startswith(\"@\")]\n",
    "\n",
    "def parse_entry(entry_text: str):\n",
    "    \"\"\"\n",
    "    Parses a BibTeX entry into a dict with common fields.\n",
    "    This is intentionally lightweight (no external packages).\n",
    "    \"\"\"\n",
    "    lines = entry_text.splitlines()\n",
    "    m = ENTRY_START_RE.match(lines[0])\n",
    "    key = m.group(1).strip() if m else \"\"\n",
    "\n",
    "    fields = {}\n",
    "    for line in lines[1:]:\n",
    "        fm = FIELD_RE.match(line)\n",
    "        if fm:\n",
    "            fname = fm.group(1).lower()\n",
    "            fval = fm.group(2).strip()\n",
    "            fields[fname] = fval\n",
    "\n",
    "    title = fields.get(\"title\", \"\")\n",
    "    year = fields.get(\"year\", \"\")\n",
    "    doi = fields.get(\"doi\", \"\")\n",
    "    booktitle = fields.get(\"booktitle\", \"\")\n",
    "    journal = fields.get(\"journal\", \"\")\n",
    "    venue = journal or booktitle\n",
    "\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"title\": title,\n",
    "        \"year\": year,\n",
    "        \"doi\": doi,\n",
    "        \"venue\": venue,\n",
    "        \"raw\": entry_text,\n",
    "    }\n",
    "\n",
    "def has_any_keyword(text: str, keywords):\n",
    "    t = text.lower()\n",
    "    return any(k.lower() in t for k in keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de96f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [f for f in BIB_FILES if not Path(f).exists()]\n",
    "if missing:\n",
    "    print(\"Missing files:\")\n",
    "    for f in missing:\n",
    "        print(\"  -\", f)\n",
    "else:\n",
    "    print(\"All bib files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca379d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "missing_doi_rows = []\n",
    "candidates = []\n",
    "\n",
    "doi_seen = set()\n",
    "title_year_seen = set()\n",
    "\n",
    "per_file_counts = {}\n",
    "\n",
    "for fname in BIB_FILES:\n",
    "    path = Path(fname)\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    entries = split_entries(text)\n",
    "    per_file_counts[fname] = len(entries)\n",
    "\n",
    "    for e in entries:\n",
    "        item = parse_entry(e)\n",
    "\n",
    "        # Dedup logic\n",
    "        doi_norm = item[\"doi\"].strip().lower()\n",
    "        title_norm = re.sub(r\"\\s+\", \" \", item[\"title\"].strip().lower())\n",
    "        ty_norm = (title_norm, item[\"year\"].strip())\n",
    "\n",
    "        is_dup = False\n",
    "        if doi_norm:\n",
    "            if doi_norm in doi_seen:\n",
    "                is_dup = True\n",
    "            doi_seen.add(doi_norm)\n",
    "        else:\n",
    "            if ty_norm in title_year_seen:\n",
    "                is_dup = True\n",
    "            title_year_seen.add(ty_norm)\n",
    "\n",
    "        # Track missing DOI\n",
    "        if not doi_norm:\n",
    "            missing_doi_rows.append(item)\n",
    "\n",
    "        # Candidate filter\n",
    "        blob = f\"{item['title']} {item['venue']}\"\n",
    "        include = has_any_keyword(blob, INCLUDE_KEYWORDS)\n",
    "        exclude = has_any_keyword(blob, EXCLUDE_KEYWORDS)\n",
    "\n",
    "        if include and not exclude and not is_dup:\n",
    "            candidates.append(item)\n",
    "\n",
    "        all_rows.append(item)\n",
    "\n",
    "print(\"Counts per file:\")\n",
    "for k, v in per_file_counts.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(f\"\\nTotal entries scanned (including duplicates across files): {len(all_rows)}\")\n",
    "print(f\"Entries missing DOI: {len(missing_doi_rows)}\")\n",
    "print(f\"Candidate count (post-dedup): {len(candidates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv = Path(\"bib_candidates.csv\")\n",
    "\n",
    "with out_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"year\", \"title\", \"venue\", \"doi\", \"key\"])\n",
    "    w.writeheader()\n",
    "    for c in sorted(candidates, key=lambda x: (x[\"year\"], x[\"title\"])):\n",
    "        w.writerow({k: c.get(k, \"\") for k in [\"year\", \"title\", \"venue\", \"doi\", \"key\"]})\n",
    "\n",
    "print(\"Wrote:\", out_csv.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a65c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a quick sample in the notebook\n",
    "for c in candidates[:25]:\n",
    "    print(f\"{c['year']} | {c['title']} | {c['venue']} | {c['doi']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_missing = Path(\"bib_missing_doi.csv\")\n",
    "with out_missing.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"year\", \"title\", \"venue\", \"key\"])\n",
    "    w.writeheader()\n",
    "    for m in sorted(missing_doi_rows, key=lambda x: (x[\"year\"], x[\"title\"])):\n",
    "        w.writerow({k: m.get(k, \"\") for k in [\"year\", \"title\", \"venue\", \"key\"]})\n",
    "\n",
    "print(\"Wrote:\", out_missing.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
